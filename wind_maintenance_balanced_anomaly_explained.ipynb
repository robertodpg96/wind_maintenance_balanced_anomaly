{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288c412a",
   "metadata": {},
   "source": [
    "\n",
    "# Wind Turbine Maintenance — Explained Notebook (Updated)\n",
    "\n",
    "This notebook mirrors the **latest training script** and explains each step in detail.  \n",
    "It covers supervised learning (SMOTE, ROC/PR, **Precision–Recall vs Threshold**), threshold sweeps, **top threshold recommendations**, and unsupervised anomaly detection.  \n",
    "It also exports **supervised alerts** (per-model and union).\n",
    "\n",
    "**Outputs are saved to:** `outputs_balanced_anomaly_nb_explained/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae100829",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Environment Setup (optional)\n",
    "**Purpose:** Ensure required libraries are available.  \n",
    "**Interpretation:** If an import error occurs later, return and run the pip line below (uncomment first).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0787c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: install dependencies if needed (uncomment if you get missing package errors)\n",
    "# !pip install -U pandas numpy scikit-learn matplotlib xgboost lightgbm imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cefce2f",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Imports & Configuration\n",
    "**Purpose:** Load numerical, plotting, and ML libraries.  \n",
    "**Details:** GBDT backend prefers **XGBoost → LightGBM → sklearn**; notebook will fall back gracefully.  \n",
    "**Interpretation:** Warnings about xgboost/lightgbm mean the sklearn fallback will be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ebce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    average_precision_score, confusion_matrix, RocCurveDisplay, PrecisionRecallDisplay,\n",
    "    precision_recall_curve\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Preferred gradient boosting backends\n",
    "XGB_AVAILABLE = False\n",
    "LGBM_AVAILABLE = False\n",
    "SKLEARN_GB_AVAILABLE = True\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    warnings.warn(\"xgboost not available; will try lightgbm or sklearn GradientBoosting.\")\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    LGBM_AVAILABLE = True\n",
    "except Exception:\n",
    "    warnings.warn(\"lightgbm not available; may fall back to sklearn.\")\n",
    "try:\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "except Exception:\n",
    "    SKLEARN_GB_AVAILABLE = False\n",
    "\n",
    "# SMOTE for class imbalance\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except Exception:\n",
    "    raise SystemExit(\"Missing dependency: imbalanced-learn. Install with: pip install imbalanced-learn\")\n",
    "\n",
    "OUTDIR = Path(\"outputs_balanced_anomaly_nb_explained\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9376c1a2",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Load Dataset & Define Target\n",
    "**Purpose:** Read the CSV and construct a clean **feature matrix (X)** and **binary target (y)**.  \n",
    "**Inputs:** `wind_turbine_maintenance_data.csv` with `Maintenance_Label`. Values > 0 are treated as maintenance (1).  \n",
    "**Outputs:** `X` (numeric features), `y` (0/1).  \n",
    "**Interpretation:** Check class balance; imbalance is expected and will be addressed with SMOTE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = os.getenv(\"WIND_DATA_PATH\", \"wind_turbine_maintenance_data.csv\")\n",
    "assert Path(DATA_PATH).exists(), f\"CSV not found at: {DATA_PATH}\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "assert 'Maintenance_Label' in df.columns, \"Expected 'Maintenance_Label' in dataset.\"\n",
    "\n",
    "y = pd.to_numeric(df['Maintenance_Label'], errors='coerce').fillna(0); y = (y > 0).astype(int)\n",
    "X = df.drop(columns=['Maintenance_Label']).select_dtypes(include=[np.number])\n",
    "\n",
    "print(\"Label counts (dataset):\\n\", y.value_counts().to_string())\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649478ce",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Train/Test Split & SMOTE (Training Only)\n",
    "**Purpose:** Create unbiased evaluation split and **balance** minority class during training.  \n",
    "**Key parameters:** `test_size=0.2`, `random_state=42`, `stratify=y`.  \n",
    "**Interpretation:** After SMOTE, class counts in `y_res` should be roughly equal. **Do not** oversample the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55906c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Label counts (train):\", np.unique(y_train, return_counts=True))\n",
    "print(\"Label counts (test) :\", np.unique(y_test, return_counts=True))\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "print(\"Resampled label counts:\", np.unique(y_res, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa23ff9",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Train Supervised Models (RF + GBDT)\n",
    "**Purpose:** Fit tree-based classifiers to predict maintenance needs.  \n",
    "**Models:** RandomForest and GBDT (XGBoost/LightGBM/sklearn fallback).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28721e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_rf():\n",
    "    return RandomForestClassifier(n_estimators=400, n_jobs=-1, random_state=42)\n",
    "\n",
    "def make_gbdt():\n",
    "    if XGB_AVAILABLE:\n",
    "        return XGBClassifier(n_estimators=600, max_depth=6, learning_rate=0.05,\n",
    "                             subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "                             objective=\"binary:logistic\", tree_method=\"hist\",\n",
    "                             random_state=42, n_jobs=-1, scale_pos_weight=1.0)\n",
    "    if LGBM_AVAILABLE:\n",
    "        return LGBMClassifier(n_estimators=700, learning_rate=0.05, subsample=0.8,\n",
    "                              colsample_bytree=0.8, reg_lambda=1.0, objective=\"binary\",\n",
    "                              random_state=42, n_jobs=-1)\n",
    "    return GradientBoostingClassifier(n_estimators=300, learning_rate=0.08, max_depth=3, random_state=42)\n",
    "\n",
    "rf = make_rf().fit(X_res, y_res)\n",
    "gb = make_gbdt().fit(X_res, y_res)\n",
    "rf, gb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c66c8e",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Evaluate Ranking and Threshold Trade-offs\n",
    "**Purpose:** Evaluate probability ranking (ROC/PR) and **select operating thresholds** using **Precision–Recall vs Threshold**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3618e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_proba(model, X):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        d = model.decision_function(X)\n",
    "        d_min, d_max = d.min(), d.max()\n",
    "        return (d - d_min) / (d_max - d_min + 1e-9)\n",
    "    return model.predict(X)\n",
    "\n",
    "y_proba_rf = get_proba(rf, X_test)\n",
    "y_proba_gb = get_proba(gb, X_test)\n",
    "\n",
    "# ROC\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "RocCurveDisplay.from_predictions(y_test, y_proba_rf, name=\"RandomForest\", ax=ax)\n",
    "RocCurveDisplay.from_predictions(y_test, y_proba_gb, name=\"GBDT\", ax=ax)\n",
    "ax.set_title(\"ROC — RF vs GBDT\"); fig.tight_layout()\n",
    "fig.savefig(OUTDIR / \"roc_both.png\", dpi=150); plt.show()\n",
    "\n",
    "# PR\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "PrecisionRecallDisplay.from_predictions(y_test, y_proba_rf, name=\"RandomForest\", ax=ax)\n",
    "PrecisionRecallDisplay.from_predictions(y_test, y_proba_gb, name=\"GBDT\", ax=ax)\n",
    "ax.set_title(\"Precision–Recall — RF vs GBDT\"); fig.tight_layout()\n",
    "fig.savefig(OUTDIR / \"pr_both.png\", dpi=150); plt.show()\n",
    "\n",
    "# PR vs threshold\n",
    "for name, probs in [(\"RandomForest\", y_proba_rf), (\"GBDT\", y_proba_gb)]:\n",
    "    p_vals, r_vals, thr_vals = precision_recall_curve(y_test, probs)\n",
    "    thr_plot = np.concatenate([thr_vals, [thr_vals[-1] if thr_vals.size else 0.5]]) if thr_vals.size else np.array([0.5])\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.plot(thr_plot, p_vals, label=\"Precision\")\n",
    "    ax.plot(thr_plot, r_vals, label=\"Recall\")\n",
    "    ax.set_xlabel(\"Threshold\"); ax.set_ylabel(\"Score\"); ax.set_title(f\"Precision–Recall vs Threshold — {name}\")\n",
    "    ax.legend(); fig.tight_layout()\n",
    "    fig.savefig(OUTDIR / f\"pr_vs_threshold_{name}.png\", dpi=150)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2b164",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Threshold Sweeps & Top 3 Threshold Recommendations\n",
    "**Purpose:** Summarize performance across thresholds and recommend **operating points**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a75271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sup_thr_sweep(y_true, y_prob):\n",
    "    rows = []\n",
    "    thresholds = np.linspace(0.05, 0.95, 19)\n",
    "    for thr in thresholds:\n",
    "        y_pred = (y_prob >= thr).astype(int)\n",
    "        rows.append({\n",
    "            \"threshold\": thr,\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "            \"f1\": f1_score(y_true, y_pred, zero_division=0)\n",
    "        })\n",
    "    df_thr = pd.DataFrame(rows)\n",
    "    best = df_thr.iloc[df_thr['f1'].values.argmax()]\n",
    "    return float(best['threshold']), df_thr\n",
    "\n",
    "def _top3_thresholds(df_thr, min_precisions=(0.3, 0.5)):\n",
    "    out = {}\n",
    "    if df_thr is None or df_thr.empty:\n",
    "        return out\n",
    "    i = df_thr['f1'].values.argmax()\n",
    "    out['best_f1_threshold'] = float(df_thr.iloc[i]['threshold'])\n",
    "    out['best_f1'] = float(df_thr.iloc[i]['f1'])\n",
    "    out['best_f1_precision'] = float(df_thr.iloc[i]['precision'])\n",
    "    out['best_f1_recall'] = float(df_thr.iloc[i]['recall'])\n",
    "    for pmin in min_precisions:\n",
    "        df_ok = df_thr[df_thr['precision'] >= pmin]\n",
    "        if len(df_ok):\n",
    "            j = df_ok['recall'].values.argmax()\n",
    "            row = df_ok.iloc[j]\n",
    "            out[f'prec>={pmin}_threshold'] = float(row['threshold'])\n",
    "            out[f'prec>={pmin}_recall'] = float(row['recall'])\n",
    "            out[f'prec>={pmin}_f1'] = float(row['f1'])\n",
    "        else:\n",
    "            out[f'prec>={pmin}_threshold'] = None\n",
    "            out[f'prec>={pmin}_recall'] = None\n",
    "            out[f'prec>={pmin}_f1'] = None\n",
    "    return out\n",
    "\n",
    "best_thr_rf, df_thr_rf = sup_thr_sweep(y_test, y_proba_rf)\n",
    "best_thr_gb, df_thr_gb = sup_thr_sweep(y_test, y_proba_gb)\n",
    "\n",
    "df_thr_rf.to_csv(OUTDIR / \"threshold_sweep_rf.csv\", index=False)\n",
    "df_thr_gb.to_csv(OUTDIR / \"threshold_sweep_gbdt.csv\", index=False)\n",
    "\n",
    "top_sup_df = pd.DataFrame([\n",
    "    {'model':'RandomForest', **_top3_thresholds(df_thr_rf)},\n",
    "    {'model':'GBDT', **_top3_thresholds(df_thr_gb)},\n",
    "]).set_index('model')\n",
    "top_sup_df.to_csv(OUTDIR / \"top_thresholds_supervised.csv\")\n",
    "top_sup_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2f89e2",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Export Supervised Alerts (per‑model and union)\n",
    "**Purpose:** Create actionable alert files by applying chosen thresholds to test predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be1611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ALERT_POLICY = \"min_precision\"   # \"best_f1\" or \"min_precision\"\n",
    "MIN_PRECISION = 0.5              # Used only when ALERT_POLICY == \"min_precision\"\n",
    "\n",
    "def _pick_thr(df_thr, policy, pmin):\n",
    "    if policy == \"best_f1\":\n",
    "        i = df_thr['f1'].values.argmax()\n",
    "        return float(df_thr.iloc[i]['threshold']), \"best_f1\"\n",
    "    df_ok = df_thr[df_thr['precision'] >= pmin]\n",
    "    if len(df_ok):\n",
    "        j = df_ok['recall'].values.argmax()\n",
    "        return float(df_ok.iloc[j]['threshold']), f\"min_precision>={pmin}\"\n",
    "    i = df_thr['f1'].values.argmax()\n",
    "    return float(df_thr.iloc[i]['threshold']), \"fallback_best_f1\"\n",
    "\n",
    "thr_rf_alert, policy_rf = _pick_thr(df_thr_rf, ALERT_POLICY, MIN_PRECISION)\n",
    "thr_gb_alert, policy_gb = _pick_thr(df_thr_gb, ALERT_POLICY, MIN_PRECISION)\n",
    "\n",
    "sup_rf = pd.DataFrame(index=X_test.index)\n",
    "sup_rf['predicted_prob'] = y_proba_rf\n",
    "sup_rf['predicted_label'] = (y_proba_rf >= thr_rf_alert).astype(int)\n",
    "sup_rf['true_label'] = y_test.values\n",
    "if \"Turbine_ID\" in df.columns:\n",
    "    sup_rf['Turbine_ID'] = df.loc[X_test.index, \"Turbine_ID\"].values\n",
    "rf_alerts = sup_rf[sup_rf['predicted_label'] == 1].copy()\n",
    "rf_alerts.to_csv(OUTDIR / \"supervised_alerts_RF.csv\", index=False)\n",
    "\n",
    "sup_gb = pd.DataFrame(index=X_test.index)\n",
    "sup_gb['predicted_prob'] = y_proba_gb\n",
    "sup_gb['predicted_label'] = (y_proba_gb >= thr_gb_alert).astype(int)\n",
    "sup_gb['true_label'] = y_test.values\n",
    "if \"Turbine_ID\" in df.columns:\n",
    "    sup_gb['Turbine_ID'] = df.loc[X_test.index, \"Turbine_ID\"].values\n",
    "gb_alerts = sup_gb[sup_gb['predicted_label'] == 1].copy()\n",
    "gb_alerts.to_csv(OUTDIR / \"supervised_alerts_GBDT.csv\", index=False)\n",
    "\n",
    "union = pd.DataFrame({\n",
    "    \"row_index\": X_test.index,\n",
    "    \"true_label\": y_test.values,\n",
    "    \"prob_rf\": y_proba_rf,\n",
    "    \"prob_gbdt\": y_proba_gb,\n",
    "    \"rf_trigger\": (y_proba_rf >= thr_rf_alert).astype(int),\n",
    "    \"gbdt_trigger\": (y_proba_gb >= thr_gb_alert).astype(int),\n",
    "})\n",
    "if \"Turbine_ID\" in df.columns:\n",
    "    union[\"Turbine_ID\"] = df.loc[X_test.index, \"Turbine_ID\"].values\n",
    "union['triggered_by'] = union.apply(lambda r: \",\".join([m for m,b in [('RF', r['rf_trigger']), ('GBDT', r['gbdt_trigger'])] if b]), axis=1)\n",
    "union_alerts = union[(union['rf_trigger']==1) | (union['gbdt_trigger']==1)].copy()\n",
    "union_alerts['rf_threshold_used'] = thr_rf_alert\n",
    "union_alerts['gbdt_threshold_used'] = thr_gb_alert\n",
    "union_alerts['rf_policy'] = policy_rf\n",
    "union_alerts['gbdt_policy'] = policy_gb\n",
    "union_alerts.to_csv(OUTDIR / \"supervised_alerts_union.csv\", index=False)\n",
    "\n",
    "thr_rf_alert, thr_gb_alert, union_alerts.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dee081",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Unsupervised Anomaly Detection (IsolationForest + One‑Class SVM)\n",
    "**Purpose:** Detect unusual behavior without labels by learning the normal operating region.  \n",
    "**Outputs:** ROC/PR curves, PR vs threshold, histograms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb4f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_norm = X_train[y_train == 0]\n",
    "\n",
    "iso = IsolationForest(n_estimators=300, contamination=0.05, random_state=42).fit(X_train_norm)\n",
    "oc  = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.05).fit(X_train_norm)\n",
    "\n",
    "iso_scores = -iso.score_samples(X_test)\n",
    "oc_scores  = -oc.decision_function(X_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "RocCurveDisplay.from_predictions(y_test, iso_scores, name=\"IsolationForest\", ax=ax)\n",
    "RocCurveDisplay.from_predictions(y_test, oc_scores, name=\"OneClassSVM\", ax=ax)\n",
    "ax.set_title(\"Anomaly ROC — IF vs OCSVM\"); fig.tight_layout()\n",
    "fig.savefig(OUTDIR / \"anomaly_roc_both.png\", dpi=150); plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "PrecisionRecallDisplay.from_predictions(y_test, iso_scores, name=\"IsolationForest\", ax=ax)\n",
    "PrecisionRecallDisplay.from_predictions(y_test, oc_scores, name=\"OneClassSVM\", ax=ax)\n",
    "ax.set_title(\"Anomaly PR — IF vs OCSVM\"); fig.tight_layout()\n",
    "fig.savefig(OUTDIR / \"anomaly_pr_both.png\", dpi=150); plt.show()\n",
    "\n",
    "for name, scores in [(\"IsolationForest\", iso_scores), (\"OneClassSVM\", oc_scores)]:\n",
    "    p_vals, r_vals, thr_vals = precision_recall_curve(y_test, scores)\n",
    "    thr_plot = np.concatenate([thr_vals, [thr_vals[-1] if thr_vals.size else scores.mean()]]) if thr_vals.size else np.array([scores.mean()])\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.plot(thr_plot, p_vals, label=\"Precision\")\n",
    "    ax.plot(thr_plot, r_vals, label=\"Recall\")\n",
    "    ax.set_xlabel(\"Score threshold\"); ax.set_ylabel(\"Score\"); ax.set_title(f\"PR vs Threshold — {name}\")\n",
    "    ax.legend(); fig.tight_layout()\n",
    "    fig.savefig(OUTDIR / f\"anomaly_pr_vs_threshold_{name}.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "for name, scores in [(\"IsolationForest\", iso_scores), (\"OneClassSVM\", oc_scores)]:\n",
    "    fig, ax = plt.subplots(figsize=(5,4))\n",
    "    ax.hist(scores, bins=40)\n",
    "    ax.set_title(f\"Anomaly Score Histogram — {name}\")\n",
    "    ax.set_xlabel(\"Score (higher = more anomalous)\"); ax.set_ylabel(\"Count\")\n",
    "    fig.tight_layout(); fig.savefig(OUTDIR / f\"anomaly_hist_{name}.png\", dpi=150); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549aa6a",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Anomaly Threshold Sweeps & Top Threshold Recommendations\n",
    "**Purpose:** Convert anomaly scores into decisions using a score cutoff; recommend operating points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a1b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def threshold_sweep_scores(y_true, scores):\n",
    "    rows = []\n",
    "    thr_values = np.linspace(np.percentile(scores, 5), np.percentile(scores, 95), 31)\n",
    "    for thr in thr_values:\n",
    "        y_pred = (scores >= thr).astype(int)\n",
    "        rows.append({\n",
    "            \"threshold\": thr,\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "            \"f1\": f1_score(y_true, y_pred, zero_division=0)\n",
    "        })\n",
    "    df_thr = pd.DataFrame(rows)\n",
    "    best = df_thr.iloc[df_thr['f1'].values.argmax()]\n",
    "    return float(best['threshold']), df_thr\n",
    "\n",
    "best_thr_iso, df_thr_iso = threshold_sweep_scores(y_test, iso_scores)\n",
    "best_thr_oc,  df_thr_oc  = threshold_sweep_scores(y_test, oc_scores)\n",
    "\n",
    "df_thr_iso.to_csv(OUTDIR / \"threshold_sweep_iso.csv\", index=False)\n",
    "df_thr_oc.to_csv(OUTDIR / \"threshold_sweep_ocsvm.csv\", index=False)\n",
    "\n",
    "def _top3_thresholds(df_thr, min_precisions=(0.3, 0.5)):\n",
    "    out = {}\n",
    "    if df_thr is None or df_thr.empty:\n",
    "        return out\n",
    "    i = df_thr['f1'].values.argmax()\n",
    "    out['best_f1_threshold'] = float(df_thr.iloc[i]['threshold'])\n",
    "    out['best_f1'] = float(df_thr.iloc[i]['f1'])\n",
    "    out['best_f1_precision'] = float(df_thr.iloc[i]['precision'])\n",
    "    out['best_f1_recall'] = float(df_thr.iloc[i]['recall'])\n",
    "    for pmin in min_precisions:\n",
    "        df_ok = df_thr[df_thr['precision'] >= pmin]\n",
    "        if len(df_ok):\n",
    "            j = df_ok['recall'].values.argmax()\n",
    "            row = df_ok.iloc[j]\n",
    "            out[f'prec>={pmin}_threshold'] = float(row['threshold'])\n",
    "            out[f'prec>={pmin}_recall'] = float(row['recall'])\n",
    "            out[f'prec>={pmin}_f1'] = float(row['f1'])\n",
    "        else:\n",
    "            out[f'prec>={pmin}_threshold'] = None\n",
    "            out[f'prec>={pmin}_recall'] = None\n",
    "            out[f'prec>={pmin}_f1'] = None\n",
    "    return out\n",
    "\n",
    "top_anom_df = pd.DataFrame([\n",
    "    {'model':'IsolationForest', **_top3_thresholds(df_thr_iso)},\n",
    "    {'model':'OneClassSVM', **_top3_thresholds(df_thr_oc)},\n",
    "]).set_index('model')\n",
    "top_anom_df.to_csv(OUTDIR / \"top_thresholds_anomaly.csv\")\n",
    "\n",
    "# Top anomalies (first 50)\n",
    "for name, scores in [(\"IsolationForest\", iso_scores), (\"OneClassSVM\", oc_scores)]:\n",
    "    top_idx = np.argsort(scores)[::-1][:50]\n",
    "    top_df = pd.DataFrame({\n",
    "        \"row_index\": X_test.index[top_idx],\n",
    "        \"score\": scores[top_idx],\n",
    "        \"true_label\": y_test.iloc[top_idx].values\n",
    "    })\n",
    "    if \"Turbine_ID\" in df.columns:\n",
    "        top_df[\"Turbine_ID\"] = df.loc[X_test.index[top_idx], \"Turbine_ID\"].values\n",
    "    top_df.to_csv(OUTDIR / f\"top_anomalies_{name}.csv\", index=False)\n",
    "\n",
    "top_anom_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0994c0",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Feature Importances\n",
    "**Purpose:** Identify which sensors/features contribute most to predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf9b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_importance(model, feature_names, title, outname):\n",
    "    if not hasattr(model, \"feature_importances_\"):\n",
    "        print(f\"No feature_importances_ for {title}.\"); return\n",
    "    imp = model.feature_importances_\n",
    "    order = np.argsort(imp)[::-1][:15]\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    ax.barh(range(len(order))[::-1], imp[order][::-1])\n",
    "    ax.set_yticks(range(len(order))[::-1])\n",
    "    ax.set_yticklabels([feature_names[i] for i in order][::-1])\n",
    "    ax.set_xlabel(\"Importance\"); ax.set_title(title)\n",
    "    fig.tight_layout(); fig.savefig(OUTDIR / outname, dpi=150); plt.show()\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "plot_importance(rf, feature_names, \"Feature Importance — RandomForest (SMOTE)\", \"feature_importance_rf_smote.png\")\n",
    "plot_importance(gb, feature_names, \"Feature Importance — GBDT (SMOTE)\", \"feature_importance_gbdt_smote.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e84b46",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Next Steps\n",
    "- Use **PR‑vs‑threshold** and **Top threshold tables** to set alert policies aligned to cost and capacity.\n",
    "- Feed `supervised_alerts_union.csv` into your SMS utility to send notifications.\n",
    "- Add temporal features and SHAP explanations for richer operational insights.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
