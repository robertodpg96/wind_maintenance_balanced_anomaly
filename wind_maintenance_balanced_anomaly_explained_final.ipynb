{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3201c024",
   "metadata": {},
   "source": [
    "\n",
    "# Wind Turbine Maintenance — **Final Explained Notebook (Step‑by‑Step)**\n",
    "\n",
    "This notebook is a **fully explained**, **decision‑oriented** walkthrough of the pipeline that powers the one‑pager report.\n",
    "For every code block, you'll see:\n",
    "- **Purpose**: why the step exists and what question it answers\n",
    "- **Inputs/Outputs**: what data goes in and what artifacts come out\n",
    "- **Key parameters**: knobs to turn and why they matter\n",
    "- **How to interpret**: what to look for in the outputs to make a decision\n",
    "\n",
    "> Tip: Run cells in order (top → bottom). Artifacts are saved to `outputs_balanced_anomaly_nb_explained/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f6158",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Environment Setup (optional)\n",
    "**Purpose:** Ensure dependencies are available, especially in a new environment.  \n",
    "**How to interpret:** If you see errors about missing packages later, come back and run this with the `!pip` line uncommented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd213b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: install dependencies if needed (uncomment if you get missing package errors)\n",
    "# !pip install -U pandas numpy scikit-learn matplotlib xgboost lightgbm imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e119aa59",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Imports & Configuration\n",
    "**Purpose:** Load the scientific Python stack and modeling libraries.  \n",
    "**Key details:**\n",
    "- We prefer **XGBoost → LightGBM → sklearn** for GBDT, depending on what's installed.\n",
    "- **SMOTE** will balance the training data to fight class imbalance.\n",
    "**How to interpret:** If you see warnings about XGBoost/LightGBM, we fall back gracefully to sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb90d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    average_precision_score, confusion_matrix, RocCurveDisplay, PrecisionRecallDisplay,\n",
    "    precision_recall_curve\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Prefer boosted trees backends in this order: XGBoost -> LightGBM -> sklearn\n",
    "XGB_AVAILABLE = False\n",
    "LGBM_AVAILABLE = False\n",
    "SKLEARN_GB_AVAILABLE = True\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    warnings.warn(\"xgboost not available; will try lightgbm or sklearn GradientBoosting.\")\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    LGBM_AVAILABLE = True\n",
    "except Exception:\n",
    "    warnings.warn(\"lightgbm not available; may fall back to sklearn.\")\n",
    "try:\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "except Exception:\n",
    "    SKLEARN_GB_AVAILABLE = False\n",
    "\n",
    "# SMOTE for class imbalance\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except Exception:\n",
    "    raise SystemExit(\"Missing dependency: imbalanced-learn. Install with: pip install imbalanced-learn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f2a92",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Load Dataset & Define Target\n",
    "**Purpose:** Read the CSV and produce a clean **feature matrix (X)** and **binary target (y)**.  \n",
    "**Inputs:** `wind_turbine_maintenance_data.csv` with a `Maintenance_Label` column.  \n",
    "**Outputs:** \n",
    "- `X`: numeric predictors (sensor features, etc.)  \n",
    "- `y`: 0 = normal, 1 = maintenance-needed (values > 0 become 1).  \n",
    "**How to interpret:** Confirm label counts; strong imbalance is common and expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03139f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = os.getenv(\"WIND_DATA_PATH\", \"wind_turbine_maintenance_data.csv\")\n",
    "assert Path(DATA_PATH).exists(), f\"CSV not found at: {DATA_PATH}\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "assert 'Maintenance_Label' in df.columns, \"Expected 'Maintenance_Label' in dataset.\"\n",
    "\n",
    "# Force binary target: maintenance if value > 0\n",
    "y = pd.to_numeric(df['Maintenance_Label'], errors='coerce').fillna(0)\n",
    "y = (y > 0).astype(int)\n",
    "\n",
    "# Keep numeric features only for tree models\n",
    "X = df.drop(columns=['Maintenance_Label']).select_dtypes(include=[np.number])\n",
    "\n",
    "print(\"Label counts (dataset):\\n\", y.value_counts().to_string())\n",
    "df.head(3)  # quick peek\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae80782",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Train/Test Split & **SMOTE** (training only)\n",
    "**Purpose:** Create an unbiased evaluation and **balance** the training set to improve learning.  \n",
    "**Key parameters:** `test_size=0.2`, `random_state=42` (reproducibility).  \n",
    "**How to interpret:** SMOTE should roughly equalize class counts in `y_res`. We never oversample the **test** set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf83c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Label counts (train):\", np.unique(y_train, return_counts=True))\n",
    "print(\"Label counts (test) :\", np.unique(y_test, return_counts=True))\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "print(\"Resampled label counts:\", np.unique(y_res, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f3363",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Train Supervised Models (RF + GBDT)\n",
    "**Purpose:** Learn to predict maintenance from historical signals.  \n",
    "**Models:**\n",
    "- **RandomForest** (robust, interpretable via importances)\n",
    "- **GBDT** (XGBoost/LightGBM/sklearn fallback) often best on tabular data  \n",
    "**How to interpret:** No printed metrics yet — that comes next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582cd3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_rf():\n",
    "    return RandomForestClassifier(n_estimators=400, n_jobs=-1, random_state=42)\n",
    "\n",
    "def make_gbdt():\n",
    "    if XGB_AVAILABLE:\n",
    "        return XGBClassifier(n_estimators=600, max_depth=6, learning_rate=0.05,\n",
    "                             subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "                             objective=\"binary:logistic\", tree_method=\"hist\",\n",
    "                             random_state=42, n_jobs=-1, scale_pos_weight=1.0)\n",
    "    if LGBM_AVAILABLE:\n",
    "        return LGBMClassifier(n_estimators=700, learning_rate=0.05, subsample=0.8,\n",
    "                              colsample_bytree=0.8, reg_lambda=1.0, objective=\"binary\",\n",
    "                              random_state=42, n_jobs=-1)\n",
    "    return GradientBoostingClassifier(n_estimators=300, learning_rate=0.08, max_depth=3, random_state=42)\n",
    "\n",
    "rf = make_rf().fit(X_res, y_res)\n",
    "gb = make_gbdt().fit(X_res, y_res)\n",
    "rf, gb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95301657",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Evaluate: ROC/PR & **Precision–Recall vs Threshold**\n",
    "**Purpose:** Understand ranking quality and choose an **operating threshold** for alerts.  \n",
    "**Outputs:**  \n",
    "- ROC/PR curves (model comparison)  \n",
    "- PR vs Threshold (for each model) to **pick a cutoff** that meets precision/recall targets.  \n",
    "**How to interpret:** Use PR‑vs‑threshold to set alert policies (e.g., *precision ≥ 0.5*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baac090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_proba(model, X):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        d = model.decision_function(X)\n",
    "        d_min, d_max = d.min(), d.max()\n",
    "        return (d - d_min) / (d_max - d_min + 1e-9)\n",
    "    return model.predict(X)\n",
    "\n",
    "y_proba_rf = get_proba(rf, X_test)\n",
    "y_proba_gb = get_proba(gb, X_test)\n",
    "\n",
    "# ROC\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "RocCurveDisplay.from_predictions(y_test, y_proba_rf, name=\"RandomForest\", ax=ax)\n",
    "RocCurveDisplay.from_predictions(y_test, y_proba_gb, name=\"GBDT\", ax=ax)\n",
    "ax.set_title(\"ROC — RF vs GBDT\"); fig.tight_layout(); plt.show()\n",
    "\n",
    "# PR\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "PrecisionRecallDisplay.from_predictions(y_test, y_proba_rf, name=\"RandomForest\", ax=ax)\n",
    "PrecisionRecallDisplay.from_predictions(y_test, y_proba_gb, name=\"GBDT\", ax=ax)\n",
    "ax.set_title(\"Precision–Recall — RF vs GBDT\"); fig.tight_layout(); plt.show()\n",
    "\n",
    "# PR vs threshold\n",
    "outdir_nb = Path(\"outputs_balanced_anomaly_nb_explained\"); outdir_nb.mkdir(parents=True, exist_ok=True)\n",
    "for name, probs in [(\"RandomForest\", y_proba_rf), (\"GBDT\", y_proba_gb)]:\n",
    "    p_vals, r_vals, thr_vals = precision_recall_curve(y_test, probs)\n",
    "    thr_plot = np.concatenate([thr_vals, [thr_vals[-1] if thr_vals.size else 0.5]]) if thr_vals.size else np.array([0.5])\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.plot(thr_plot, p_vals, label=\"Precision\")\n",
    "    ax.plot(thr_plot, r_vals, label=\"Recall\")\n",
    "    ax.set_xlabel(\"Threshold\"); ax.set_ylabel(\"Score\"); ax.set_title(f\"Precision–Recall vs Threshold — {name}\")\n",
    "    ax.legend(); fig.tight_layout()\n",
    "    fig.savefig(outdir_nb / f\"pr_vs_threshold_{name}.png\", dpi=150)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b78470e",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Threshold Sweeps + **Top Threshold Picks** (Supervised)\n",
    "**Purpose:** Convert probabilities → decisions using a **threshold**, and summarize recommended operating points.  \n",
    "**Outputs:**  \n",
    "- `threshold_sweep_rf.csv`, `threshold_sweep_gbdt.csv`  \n",
    "- `top_thresholds_supervised.csv` with: **best F1**, and best recall s.t. **precision ≥ 0.3** and **≥ 0.5**  \n",
    "**How to interpret:** Choose the row that matches your false‑alarm tolerance and recall needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sup_thr_sweep(y_true, y_prob):\n",
    "    rows = []\n",
    "    thresholds = np.linspace(0.05, 0.95, 19)\n",
    "    for thr in thresholds:\n",
    "        y_pred = (y_prob >= thr).astype(int)\n",
    "        rows.append({\n",
    "            \"threshold\": thr,\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "            \"f1\": f1_score(y_true, y_pred, zero_division=0)\n",
    "        })\n",
    "    df_thr = pd.DataFrame(rows)\n",
    "    best = df_thr.iloc[df_thr['f1'].values.argmax()]\n",
    "    return float(best['threshold']), df_thr\n",
    "\n",
    "def _top3_thresholds(df_thr, min_precisions=(0.3, 0.5)):\n",
    "    out = {}\n",
    "    if df_thr is None or df_thr.empty:\n",
    "        return out\n",
    "    i = df_thr['f1'].values.argmax()\n",
    "    out['best_f1_threshold'] = float(df_thr.iloc[i]['threshold'])\n",
    "    out['best_f1'] = float(df_thr.iloc[i]['f1'])\n",
    "    out['best_f1_precision'] = float(df_thr.iloc[i]['precision'])\n",
    "    out['best_f1_recall'] = float(df_thr.iloc[i]['recall'])\n",
    "    for pmin in min_precisions:\n",
    "        df_ok = df_thr[df_thr['precision'] >= pmin]\n",
    "        if len(df_ok):\n",
    "            j = df_ok['recall'].values.argmax()\n",
    "            row = df_ok.iloc[j]\n",
    "            out[f'prec>={pmin}_threshold'] = float(row['threshold'])\n",
    "            out[f'prec>={pmin}_recall'] = float(row['recall'])\n",
    "            out[f'prec>={pmin}_f1'] = float(row['f1'])\n",
    "        else:\n",
    "            out[f'prec>={pmin}_threshold'] = None\n",
    "            out[f'prec>={pmin}_recall'] = None\n",
    "            out[f'prec>={pmin}_f1'] = None\n",
    "    return out\n",
    "\n",
    "best_thr_rf, df_thr_rf = sup_thr_sweep(y_test, y_proba_rf)\n",
    "best_thr_gb, df_thr_gb = sup_thr_sweep(y_test, y_proba_gb)\n",
    "\n",
    "# Save sweeps + top picks\n",
    "outdir_nb.mkdir(parents=True, exist_ok=True)\n",
    "df_thr_rf.to_csv(outdir_nb / \"threshold_sweep_rf.csv\", index=False)\n",
    "df_thr_gb.to_csv(outdir_nb / \"threshold_sweep_gbdt.csv\", index=False)\n",
    "\n",
    "top_sup_df = pd.DataFrame([\n",
    "    {'model':'RandomForest', **_top3_thresholds(df_thr_rf)},\n",
    "    {'model':'GBDT', **_top3_thresholds(df_thr_gb)},\n",
    "]).set_index('model')\n",
    "top_sup_df.to_csv(outdir_nb / \"top_thresholds_supervised.csv\")\n",
    "top_sup_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c77b8f2",
   "metadata": {},
   "source": [
    "\n",
    "## 7) **Unsupervised Anomaly Detection** (IsolationForest + One‑Class SVM) & PR‑vs‑Threshold\n",
    "**Purpose:** Flag unusual behavior without labels; evaluate against labels **if available**.  \n",
    "**Why train on normal only?** Anomalies are by definition rare and varied — learning the **normal manifold** is more stable.  \n",
    "**Outputs:** ROC/PR, PR‑vs‑threshold plots for score cutoffs.\n",
    "**How to interpret:** Use PR‑AUC and PR‑vs‑threshold to set **alerting policy**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85f51b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train on *normal* training data only\n",
    "X_train_norm = X_train[y_train == 0]\n",
    "\n",
    "iso = IsolationForest(n_estimators=300, contamination=0.05, random_state=42).fit(X_train_norm)\n",
    "oc  = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.05).fit(X_train_norm)\n",
    "\n",
    "# Higher = more anomalous\n",
    "iso_scores = -iso.score_samples(X_test)\n",
    "oc_scores  = -oc.decision_function(X_test)\n",
    "\n",
    "# ROC/PR\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "RocCurveDisplay.from_predictions(y_test, iso_scores, name=\"IsolationForest\", ax=ax)\n",
    "RocCurveDisplay.from_predictions(y_test, oc_scores, name=\"OneClassSVM\", ax=ax)\n",
    "ax.set_title(\"Anomaly ROC — IF vs OCSVM\"); fig.tight_layout(); plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "PrecisionRecallDisplay.from_predictions(y_test, iso_scores, name=\"IsolationForest\", ax=ax)\n",
    "PrecisionRecallDisplay.from_predictions(y_test, oc_scores, name=\"OneClassSVM\", ax=ax)\n",
    "ax.set_title(\"Anomaly PR — IF vs OCSVM\"); fig.tight_layout(); plt.show()\n",
    "\n",
    "# PR vs threshold (anomaly scores)\n",
    "for name, scores in [(\"IsolationForest\", iso_scores), (\"OneClassSVM\", oc_scores)]:\n",
    "    p_vals, r_vals, thr_vals = precision_recall_curve(y_test, scores)\n",
    "    thr_plot = np.concatenate([thr_vals, [thr_vals[-1] if thr_vals.size else scores.mean()]]) if thr_vals.size else np.array([scores.mean()])\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.plot(thr_plot, p_vals, label=\"Precision\")\n",
    "    ax.plot(thr_plot, r_vals, label=\"Recall\")\n",
    "    ax.set_xlabel(\"Score threshold\"); ax.set_ylabel(\"Score\"); ax.set_title(f\"PR vs Threshold — {name}\")\n",
    "    ax.legend(); fig.tight_layout()\n",
    "    fig.savefig(outdir_nb / f\"anomaly_pr_vs_threshold_{name}.png\", dpi=150)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67bf38c",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Threshold Sweeps + **Top Threshold Picks** (Anomaly)\n",
    "**Purpose:** Convert anomaly scores → decisions using a score cutoff; summarize recommended cutoffs.  \n",
    "**Outputs:**  \n",
    "- `threshold_sweep_iso.csv`, `threshold_sweep_ocsvm.csv`  \n",
    "- `top_thresholds_anomaly.csv` with **best F1** and precision‑constrained picks  \n",
    "**How to interpret:** Lower cutoffs increase recall (more alerts); higher cutoffs increase precision (fewer alerts).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def threshold_sweep_scores(y_true, scores):\n",
    "    rows = []\n",
    "    thr_values = np.linspace(np.percentile(scores, 5), np.percentile(scores, 95), 31)\n",
    "    for thr in thr_values:\n",
    "        y_pred = (scores >= thr).astype(int)\n",
    "        rows.append({\n",
    "            \"threshold\": thr,\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "            \"f1\": f1_score(y_true, y_pred, zero_division=0)\n",
    "        })\n",
    "    df_thr = pd.DataFrame(rows)\n",
    "    best = df_thr.iloc[df_thr['f1'].values.argmax()]\n",
    "    return float(best['threshold']), df_thr\n",
    "\n",
    "best_thr_iso, df_thr_iso = threshold_sweep_scores(y_test, iso_scores)\n",
    "best_thr_oc,  df_thr_oc  = threshold_sweep_scores(y_test, oc_scores)\n",
    "\n",
    "df_thr_iso.to_csv(outdir_nb / \"threshold_sweep_iso.csv\", index=False)\n",
    "df_thr_oc.to_csv(outdir_nb / \"threshold_sweep_ocsvm.csv\", index=False)\n",
    "\n",
    "def _top3_thresholds(df_thr, min_precisions=(0.3, 0.5)):\n",
    "    out = {}\n",
    "    if df_thr is None or df_thr.empty:\n",
    "        return out\n",
    "    i = df_thr['f1'].values.argmax()\n",
    "    out['best_f1_threshold'] = float(df_thr.iloc[i]['threshold'])\n",
    "    out['best_f1'] = float(df_thr.iloc[i]['f1'])\n",
    "    out['best_f1_precision'] = float(df_thr.iloc[i]['precision'])\n",
    "    out['best_f1_recall'] = float(df_thr.iloc[i]['recall'])\n",
    "    for pmin in min_precisions:\n",
    "        df_ok = df_thr[df_thr['precision'] >= pmin]\n",
    "        if len(df_ok):\n",
    "            j = df_ok['recall'].values.argmax()\n",
    "            row = df_ok.iloc[j]\n",
    "            out[f'prec>={pmin}_threshold'] = float(row['threshold'])\n",
    "            out[f'prec>={pmin}_recall'] = float(row['recall'])\n",
    "            out[f'prec>={pmin}_f1'] = float(row['f1'])\n",
    "        else:\n",
    "            out[f'prec>={pmin}_threshold'] = None\n",
    "            out[f'prec>={pmin}_recall'] = None\n",
    "            out[f'prec>={pmin}_f1'] = None\n",
    "    return out\n",
    "\n",
    "top_anom_df = pd.DataFrame([\n",
    "    {'model':'IsolationForest', **_top3_thresholds(df_thr_iso)},\n",
    "    {'model':'OneClassSVM', **_top3_thresholds(df_thr_oc)},\n",
    "]).set_index('model')\n",
    "top_anom_df.to_csv(outdir_nb / \"top_thresholds_anomaly.csv\")\n",
    "top_anom_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb801f5b",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Export Per‑Row Scores & Top Anomalies\n",
    "**Purpose:** Produce tables for inspection tickets and dashboards.  \n",
    "**Outputs:**  \n",
    "- `anomaly_scores_test.csv` — per row scores and true label  \n",
    "- `top_anomalies_*.csv` — top 50 highest‑scored anomalies per method  \n",
    "**How to interpret:** Start with the highest scores; cross‑check **Turbine_ID** if present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c27de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predictions at best thresholds (for reference)\n",
    "y_pred_iso = (iso_scores >= best_thr_iso).astype(int)\n",
    "y_pred_oc  = (oc_scores  >= best_thr_oc).astype(int)\n",
    "\n",
    "# Combined scores\n",
    "combined_scores = pd.DataFrame({\n",
    "    \"row_index\": X_test.index,\n",
    "    \"true_label\": y_test.values,\n",
    "    \"iso_score\": iso_scores,\n",
    "    \"ocsvm_score\": oc_scores\n",
    "}).sort_values(\"iso_score\", ascending=False)\n",
    "\n",
    "if \"Turbine_ID\" in df.columns:\n",
    "    combined_scores[\"Turbine_ID\"] = df.loc[X_test.index, \"Turbine_ID\"]\n",
    "\n",
    "combined_scores.to_csv(outdir_nb / \"anomaly_scores_test.csv\", index=False)\n",
    "\n",
    "# Top anomalies per method\n",
    "for name, scores in [(\"IsolationForest\", iso_scores), (\"OneClassSVM\", oc_scores)]:\n",
    "    top_idx = np.argsort(scores)[::-1][:50]\n",
    "    top_df = pd.DataFrame({\n",
    "        \"row_index\": X_test.index[top_idx],\n",
    "        \"score\": scores[top_idx],\n",
    "        \"true_label\": y_test.iloc[top_idx].values\n",
    "    })\n",
    "    if \"Turbine_ID\" in df.columns:\n",
    "        top_df[\"Turbine_ID\"] = df.loc[X_test.index[top_idx], \"Turbine_ID\"].values\n",
    "    top_df.to_csv(outdir_nb / f\"top_anomalies_{name}.csv\", index=False)\n",
    "\n",
    "combined_scores.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8311a5",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Feature Importances (What Signals Drive Decisions?)\n",
    "**Purpose:** Identify the most influential sensors/features driving predictions.  \n",
    "**How to interpret:** Features that appear as **top‑ranked** across both RF and GBDT deserve operational focus (QA, spare parts, tighter alarms).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9021c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_importance(model, feature_names, title, outname):\n",
    "    if not hasattr(model, \"feature_importances_\"):\n",
    "        print(f\"No feature_importances_ for {title}.\"); return\n",
    "    imp = model.feature_importances_\n",
    "    order = np.argsort(imp)[::-1][:15]\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    ax.barh(range(len(order))[::-1], imp[order][::-1])\n",
    "    ax.set_yticks(range(len(order))[::-1])\n",
    "    ax.set_yticklabels([feature_names[i] for i in order][::-1])\n",
    "    ax.set_xlabel(\"Importance\"); ax.set_title(title)\n",
    "    fig.tight_layout(); fig.savefig(outdir_nb / outname, dpi=150); plt.show()\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "plot_importance(rf, feature_names, \"Feature Importance — RandomForest (SMOTE)\", \"feature_importance_rf_smote.png\")\n",
    "plot_importance(gb, feature_names, \"Feature Importance — GBDT (SMOTE)\", \"feature_importance_gbdt_smote.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9217fd6c",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Next Steps (How to Use These Results)\n",
    "- Use **PR‑vs‑threshold** and **Top threshold picks** to set alert policies (e.g., *precision ≥ 0.5* or *maximize F1*).\n",
    "- Track **precision/recall** weekly after deployment and adjust thresholds as capacity and costs evolve.\n",
    "- Add **temporal features** (rolling means, rates of change) to capture trends for better separability.\n",
    "- Consider **SHAP** for per‑prediction explanations when presenting to operators.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
